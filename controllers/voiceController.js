const axios = require('axios');
const OpenAI = require('openai');
const Lesson = require('../models/lesson');
const User = require('../models/user');
const { ElevenLabsClient } = require("@elevenlabs/elevenlabs-js");
const { extractExerciseContent, buildVoiceAssistantContext } = require('../utils/exerciseContentExtractor');
require('dotenv').config();

// String similarity using Levenshtein distance
const stringSimilarity = require('string-similarity');  // You may need to install this package

// Initialize OpenAI client
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Initialize ElevenLabs Client
const elevenlabs = new ElevenLabsClient({
  apiKey: process.env.ELEVENLABS_API_KEY,
});

// ============================================
// STREAM AUDIO - Low Latency TTS Streaming
// ============================================

// This endpoint streams audio directly from ElevenLabs to the client
// Used after analyzeLessonForSpeech returns the explanation text
exports.streamAudio = async (req, res) => {
  try {
    const { text, voiceId } = req.body;

    if (!text || text.trim().length === 0) {
      return res.status(400).json({
        success: false,
        error: 'Ð¢ÐµÐºÑÑ‚ Ð´Ð»Ñ Ð¾Ð·Ð²ÑƒÑ‡ÐºÐ¸ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚'
      });
    }

    if (!process.env.ELEVENLABS_API_KEY) {
      return res.status(500).json({
        success: false,
        error: 'ElevenLabs API key Ð½Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½'
      });
    }

    // Use environment variable or fallback to a default voice (Rachel)
    const VOICE_ID = voiceId || process.env.ELEVENLABS_VOICE_ID || '21m00Tcm4TlvDq8ikWAM';
    const API_KEY = process.env.ELEVENLABS_API_KEY;

    // ==========================================
    // ðŸ‡ºðŸ‡¿ UZBEK VOICE HANDLING
    // ==========================================
    if (req.body.language === 'uz') {
      const UZBEK_API_KEY = process.env.UZBEK_VOICE_API_KEY;

      console.log('[VoiceController] Uzbek Request:', {
        textLength: text.length,
        hasKey: !!UZBEK_API_KEY,
        keyLength: UZBEK_API_KEY ? UZBEK_API_KEY.length : 0
      });

      if (!UZBEK_API_KEY) {
        console.warn('âš ï¸ Uzbek Voice API key missing in process.env');
        return res.status(500).json({
          success: false,
          error: 'Uzbek Voice API key not configured on server',
          envKeys: Object.keys(process.env).filter(k => k.includes('API_KEY')) // Debug: list available keys
        });
      }

      // UzbekVoice.ai API Implementation
      const response = await axios({
        method: 'post',
        url: 'https://uzbekvoice.ai/api/v1/tts',
        data: {
          text: text,
          model: 'lola', // Female voice
          blocking: 'true', // Wait for audio
          // webhook_notification_url: '...' // Not needed for blocking
        },
        headers: {
          'Authorization': UZBEK_API_KEY, // User provided format: 'Authorization: [API_KEY]'
          'Content-Type': 'application/json',
        },
        responseType: 'stream'
      });

      // Check if response is JSON (error or url) or Audio
      // Assuming blocking=true returns audio stream directly based on standard TTS behavior
      // If it returns JSON with a URL, we might need to fetch it. 
      // For now, we pipe the response.

      res.set({
        'Content-Type': 'audio/mpeg', // Assuming MP3/WAV
        'Transfer-Encoding': 'chunked',
        'Cache-Control': 'no-cache'
      });

      response.data.pipe(res);
      return;
    }

    // ==========================================
    // ðŸ‡ºðŸ‡¸/ðŸ‡·ðŸ‡º ELEVENLABS HANDLING (Default)
    // ==========================================

    // Call ElevenLabs streaming API
    const response = await axios({
      method: 'post',
      url: `https://api.elevenlabs.io/v1/text-to-speech/${VOICE_ID}/stream`,
      data: {
        text: text,
        model_id: 'eleven_turbo_v2_5', // Turbo v2.5 is faster and supports better multilingual emotion
        voice_settings: {
          stability: 0.4,           // Lower stability = more emotion/variability (less robotic)
          similarity_boost: 0.6,    // Slightly lower similarity to allow more natural intonation
          style: 0.5,               // Increase style for more expressiveness
          use_speaker_boost: true   // Enhanced speaker clarity
        }
      },
      headers: {
        'Accept': 'audio/mpeg',
        'xi-api-key': API_KEY,
        'Content-Type': 'application/json',
      },
      responseType: 'stream' // CRITICAL: This enables streaming
    });

    // Set headers so the browser knows it's an audio stream
    res.set({
      'Content-Type': 'audio/mpeg',
      'Transfer-Encoding': 'chunked',
      'Cache-Control': 'no-cache'
    });

    // Pipe the audio stream directly to the client (no buffering delay)
    response.data.pipe(res);

  } catch (error) {
    let errorMessage = error.message;
    let errorDetails = null;

    // Handle Axios stream error response
    if (error.response && error.response.data) {
      try {
        if (typeof error.response.data.read === 'function' || error.response.data instanceof require('stream').Stream) {
          // It's a stream, read it to get the error message
          const chunks = [];
          for await (const chunk of error.response.data) {
            chunks.push(chunk);
          }
          const errorBody = Buffer.concat(chunks).toString('utf8');
          try {
            const jsonError = JSON.parse(errorBody);
            errorMessage = jsonError.detail?.message || jsonError.message || errorBody;
            errorDetails = jsonError;
          } catch (e) {
            errorMessage = errorBody;
          }
        } else {
          errorMessage = JSON.stringify(error.response.data);
        }
      } catch (readError) {
        console.error('Error reading error stream:', readError);
      }
    }

    console.error('âŒ Voice Stream Error:', errorMessage);

    // Don't try to send JSON if headers already sent (streaming started)
    if (!res.headersSent) {
      res.status(500).json({
        success: false,
        error: 'ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑÐ¸Ð½Ñ‚ÐµÐ·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ€ÐµÑ‡ÑŒ',
        details: errorMessage, // Exposed for debugging
        fullError: errorDetails,
        apiKeyConfigured: !!process.env.ELEVENLABS_API_KEY
      });
    }
  }
};

// ============================================
// INIT VOICE SESSION - Conversational AI Widget
// ============================================

exports.initVoiceSession = async (req, res) => {
  try {
    const { lessonId, currentStepId, currentStepIndex, language = 'en' } = req.body;

    // 1. Fetch the lesson content with .lean() for plain JS objects
    const lesson = await Lesson.findById(lessonId).lean();
    if (!lesson) return res.status(404).json({ message: "Lesson not found" });

    // 2. Find the current step by ID or index
    let currentStep = null;
    if (currentStepId) {
      currentStep = lesson.steps.find(s => s._id?.toString() === currentStepId || s.id === currentStepId);
    }
    if (!currentStep && currentStepIndex !== undefined) {
      currentStep = lesson.steps[currentStepIndex];
    }

    // 3. Extract exercise content using the new extractor
    const lessonTitle = typeof lesson.lessonName === 'string'
      ? lesson.lessonName
      : (lesson.lessonName?.[language] || lesson.lessonName?.en || lesson.lessonName?.ru || lesson.title);

    const exerciseContext = buildVoiceAssistantContext({
      step: currentStep,
      lessonTitle,
      language,
      userProgress: {
        currentStepIndex: currentStepIndex || lesson.steps.indexOf(currentStep),
        totalSteps: lesson.steps.length
      }
    });

    console.log('ðŸŽ¤ [Voice Session] Exercise Context Built:', {
      lessonId,
      stepType: currentStep?.type,
      contextLength: exerciseContext.length
    });

    // 4. Build language-specific names for the AI instruction
    const languageNames = {
      en: 'English',
      ru: 'Russian',
      uz: 'Uzbek',
      es: 'Spanish'
    };
    const targetLanguage = languageNames[language] || 'English';

    // 5. Generate a context-aware "Speech Script" via OpenAI
    const systemPrompt = `You are an AI tutor voice assistant helping a student with their lesson.
You have access to the current exercise the student is viewing.
Based on this context, create a brief, engaging introduction or hint (under 100 words).
Do NOT give away the answer directly - guide the student to discover it themselves.

CRITICAL INSTRUCTION:
The user is currently speaking in **${targetLanguage}**.
You MUST reply in **${targetLanguage}**, even if the user's code or content contains other languages.

${exerciseContext}`;

    // Call OpenAI using official package
    const aiResponse = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [
        { role: "system", content: systemPrompt },
        { role: "user", content: "Introduce this exercise to me or give me a helpful hint to get started." }
      ],
      max_tokens: 200
    });

    const aiText = aiResponse.choices[0].message.content;

    // 5. Create a Signed Session with ElevenLabs Conversational AI
    // This allows the frontend to connect without exposing your API Key
    const sessionResponse = await axios.post(
      `https://api.elevenlabs.io/v1/convai/conversation/get_signed_url?agent_id=${process.env.ELEVENLABS_AGENT_ID}`,
      {},
      { headers: { 'xi-api-key': process.env.ELEVENLABS_API_KEY } }
    );

    res.json({
      signedUrl: sessionResponse.data.signed_url,
      script: aiText,
      exerciseContext: exerciseContext // Include context for frontend debugging/display
    });

  } catch (error) {
    console.error("Voice Init Error:", error);
    res.status(500).json({ error: "Failed to initialize Elya" });
  }
};

// ============================================
// GET EXERCISE CONTEXT - Extract context for current step
// ============================================

/**
 * Returns the extracted exercise context for a given step.
 * Useful for frontend to display what the AI "sees" or for debugging.
 * Does not initialize a voice session or call external APIs.
 */
exports.getExerciseContext = async (req, res) => {
  try {
    const { lessonId, currentStepId, currentStepIndex, language = 'en' } = req.body;

    // 1. Fetch the lesson content with .lean() for plain JS objects
    const lesson = await Lesson.findById(lessonId).lean();
    if (!lesson) return res.status(404).json({ message: "Lesson not found" });

    // 2. Find the current step by ID or index
    let currentStep = null;
    if (currentStepId) {
      currentStep = lesson.steps.find(s => s._id?.toString() === currentStepId || s.id === currentStepId);
    }
    if (!currentStep && currentStepIndex !== undefined) {
      currentStep = lesson.steps[currentStepIndex];
    }

    if (!currentStep) {
      return res.status(404).json({ message: "Step not found" });
    }

    // 3. Get localized lesson title
    const lessonTitle = typeof lesson.lessonName === 'string'
      ? lesson.lessonName
      : (lesson.lessonName?.[language] || lesson.lessonName?.en || lesson.lessonName?.ru || lesson.title);

    // 4. Extract exercise content using the new extractor
    const exerciseContent = extractExerciseContent(currentStep, language);
    const fullContext = buildVoiceAssistantContext({
      step: currentStep,
      lessonTitle,
      language,
      userProgress: {
        currentStepIndex: currentStepIndex ?? lesson.steps.indexOf(currentStep),
        totalSteps: lesson.steps.length
      }
    });

    console.log('ðŸ“ [Exercise Context] Extracted:', {
      lessonId,
      stepType: currentStep?.type,
      contentLength: exerciseContent.length
    });

    res.json({
      success: true,
      exerciseContent,
      fullContext,
      stepType: currentStep.type,
      stepTitle: typeof currentStep.title === 'string'
        ? currentStep.title
        : (currentStep.title?.[language] || currentStep.title?.en || ''),
      lessonTitle
    });

  } catch (error) {
    console.error("Get Exercise Context Error:", error);
    res.status(500).json({ error: "Failed to extract exercise context", details: error.message });
  }
};

// ============================================
// PROCESS VOICE QUERY - Handle voice queries with exercise context
// ============================================

/**
 * Processes a voice/text query from the user with full exercise context.
 * Returns AI response that can be converted to speech.
 */
exports.processVoiceQuery = async (req, res) => {
  try {
    const {
      lessonId,
      currentStepId,
      currentStepIndex,
      query,
      language = 'en',
      conversationHistory = []
    } = req.body;

    if (!query || query.trim().length === 0) {
      return res.status(400).json({ error: "Query is required" });
    }

    // 1. Fetch the lesson content with .lean() for plain JS objects
    const lesson = await Lesson.findById(lessonId).lean();
    if (!lesson) return res.status(404).json({ message: "Lesson not found" });

    // 2. Find the current step
    let currentStep = null;
    if (currentStepId) {
      currentStep = lesson.steps.find(s => s._id?.toString() === currentStepId || s.id === currentStepId);
    }
    if (!currentStep && currentStepIndex !== undefined) {
      currentStep = lesson.steps[currentStepIndex];
    }

    // 3. Get localized lesson title
    const lessonTitle = typeof lesson.lessonName === 'string'
      ? lesson.lessonName
      : (lesson.lessonName?.[language] || lesson.lessonName?.en || lesson.lessonName?.ru || lesson.title);

    // 4. Build full exercise context
    const exerciseContext = buildVoiceAssistantContext({
      step: currentStep,
      lessonTitle,
      language,
      userProgress: {
        currentStepIndex: currentStepIndex ?? (currentStep ? lesson.steps.indexOf(currentStep) : 0),
        totalSteps: lesson.steps.length
      }
    });

    console.log('ðŸŽ™ï¸ [Voice Query] Processing:', {
      lessonId,
      stepType: currentStep?.type,
      queryLength: query.length,
      historyLength: conversationHistory.length
    });

    // 5. Build system prompt with exercise context
    const languageNames = {
      en: 'English',
      ru: 'Russian',
      uz: 'Uzbek',
      es: 'Spanish'
    };
    const targetLanguage = languageNames[language] || 'English';

    const systemPrompt = `You are Elya, a friendly and helpful AI tutor voice assistant.
You are helping a student with their lesson.

CRITICAL INSTRUCTION:
The user is currently speaking in **${targetLanguage}**.
You MUST reply in **${targetLanguage}**, even if the user's code or content contains other languages.

${exerciseContext}

IMPORTANT GUIDELINES:
- If the user asks for help, guide them toward the answer without giving it away directly.
- If the user is stuck, provide hints based on the exercise content above.
- If the user asks "what do I need to do?", explain the task clearly using the exercise details.
- Keep responses concise and suitable for voice (under 150 words).
- Be encouraging and supportive.
- If the user seems frustrated, offer a bigger hint or break down the problem.`;

    // 6. Build messages array with conversation history
    const messages = [
      { role: "system", content: systemPrompt },
      ...conversationHistory.slice(-10), // Keep last 10 messages for context
      { role: "user", content: query }
    ];

    // 7. Call OpenAI
    const aiResponse = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages,
      max_tokens: 300,
      temperature: 0.7
    });

    const responseText = aiResponse.choices[0].message.content;

    res.json({
      success: true,
      response: responseText,
      exerciseContext, // Include for debugging/display
      stepType: currentStep?.type
    });

  } catch (error) {
    console.error("Voice Query Error:", error);
    res.status(500).json({ error: "Failed to process voice query", details: error.message });
  }
};

// ============================================
// VERIFY VOICE ANSWER - Compare spoken answer to correct answer
// ============================================

/**
 * Verifies a voice answer against the expected correct answer using fuzzy matching.
 * This accounts for speech-to-text inaccuracies by using similarity scoring.
 */
exports.verifyVoiceAnswer = async (req, res) => {
  try {
    const { transcript, correctAnswer, language = 'en', threshold = 0.85 } = req.body;

    if (!transcript || !correctAnswer) {
      return res.status(400).json({
        success: false,
        error: 'Both transcript and correctAnswer are required'
      });
    }

    console.log('ðŸŽ¤ [Voice Verify] Comparing:', {
      transcript: transcript.substring(0, 50),
      correctAnswer: correctAnswer.substring(0, 50),
      threshold
    });

    // Normalize both strings for comparison
    const normalizedTranscript = normalizeForComparison(transcript, language);
    const normalizedCorrect = normalizeForComparison(correctAnswer, language);

    // Calculate similarity
    let similarity;
    try {
      // Try using string-similarity package if available
      similarity = stringSimilarity.compareTwoStrings(normalizedTranscript, normalizedCorrect);
    } catch (e) {
      // Fallback to built-in Levenshtein distance calculation
      similarity = calculateSimilarity(normalizedTranscript, normalizedCorrect);
    }

    const correct = similarity >= threshold;

    // Generate helpful feedback
    let feedback = '';
    if (correct) {
      feedback = getFeedbackMessage('correct', language);
    } else if (similarity >= 0.6) {
      feedback = getFeedbackMessage('close', language);
    } else {
      feedback = getFeedbackMessage('incorrect', language);
    }

    console.log('ðŸŽ¤ [Voice Verify] Result:', { similarity, correct, threshold });

    res.json({
      success: true,
      correct,
      similarity,
      feedback,
      normalizedTranscript,
      normalizedCorrect
    });

  } catch (error) {
    console.error('Voice Verify Error:', error);
    res.status(500).json({
      success: false,
      error: 'Failed to verify voice answer',
      details: error.message
    });
  }
};

/**
 * Normalize text for comparison:
 * - Lowercase
 * - Remove punctuation
 * - Normalize whitespace
 * - Handle language-specific normalizations
 */
function normalizeForComparison(text, language = 'en') {
  if (!text) return '';

  let normalized = text
    .toLowerCase()
    .trim()
    // Remove common punctuation
    .replace(/[.,!?;:'"()\[\]{}]/g, '')
    // Normalize whitespace
    .replace(/\s+/g, ' ')
    // Remove leading/trailing spaces
    .trim();

  // Language-specific normalizations
  if (language === 'en') {
    // Handle common speech-to-text variations
    normalized = normalized
      .replace(/\bthe\b/g, '') // Articles often missed/added
      .replace(/\ba\b/g, '')
      .replace(/\ban\b/g, '')
      .replace(/\s+/g, ' ')
      .trim();
  }

  return normalized;
}

/**
 * Calculate string similarity using Levenshtein distance
 * Returns a value between 0 and 1
 */
function calculateSimilarity(str1, str2) {
  if (str1 === str2) return 1;
  if (!str1 || !str2) return 0;

  const len1 = str1.length;
  const len2 = str2.length;

  // Create distance matrix
  const matrix = Array(len2 + 1).fill(null).map(() => Array(len1 + 1).fill(null));

  for (let i = 0; i <= len1; i++) matrix[0][i] = i;
  for (let j = 0; j <= len2; j++) matrix[j][0] = j;

  for (let j = 1; j <= len2; j++) {
    for (let i = 1; i <= len1; i++) {
      const indicator = str1[i - 1] === str2[j - 1] ? 0 : 1;
      matrix[j][i] = Math.min(
        matrix[j][i - 1] + 1,     // deletion
        matrix[j - 1][i] + 1,     // insertion
        matrix[j - 1][i - 1] + indicator // substitution
      );
    }
  }

  const distance = matrix[len2][len1];
  const maxLen = Math.max(len1, len2);

  return 1 - (distance / maxLen);
}

/**
 * Get localized feedback messages
 */
function getFeedbackMessage(type, language = 'en') {
  const messages = {
    en: {
      correct: 'Correct! Well done!',
      close: 'Almost there! Try saying it more clearly.',
      incorrect: 'Not quite right. Try again!'
    },
    ru: {
      correct: 'ÐŸÑ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾! ÐžÑ‚Ð»Ð¸Ñ‡Ð½Ð¾!',
      close: 'ÐŸÐ¾Ñ‡Ñ‚Ð¸! ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ ÑÐºÐ°Ð·Ð°Ñ‚ÑŒ Ñ‡ÐµÑ‚Ñ‡Ðµ.',
      incorrect: 'ÐÐµ ÑÐ¾Ð²ÑÐµÐ¼ Ð²ÐµÑ€Ð½Ð¾. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ ÐµÑ‰Ðµ Ñ€Ð°Ð·!'
    },
    uz: {
      correct: "To'g'ri! Zo'r!",
      close: "Deyarli! Aniqroq aytib ko'ring.",
      incorrect: "Noto'g'ri. Qaytadan urinib ko'ring!"
    }
  };

  return messages[language]?.[type] || messages.en[type];
}